---
title: "Practical Machine Learning Course Project"
author: "Jessica Ramos"
date: "February 11, 2016"
output: 
  html_document: 
    keep_md: yes
---

##Introduction

"Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it."

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: 

&nbsp;&nbsp;&nbsp;&nbsp;**Class A:** exactly according to the specification

&nbsp;&nbsp;&nbsp;&nbsp;**Class B:** throwing the elbows to the front

&nbsp;&nbsp;&nbsp;&nbsp;**Class C:** lifting the dumbbell only halfway

&nbsp;&nbsp;&nbsp;&nbsp;**Class D:** lowering the dumbbell only halfway

&nbsp;&nbsp;&nbsp;&nbsp;**Class E:** throwing the hips to the front

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

##Question
Using data from accelerometers on the belt, forearm, arm, and dumbell of the 6 participants, can we create a model to predict what way they performed the barbell lift (the classe variable containing A-E)?

##Data
The training data for this project are available here:
    https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
    https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

First, we read in the training and testing datasets:
```{r, cache=TRUE}
#import datasets
trainData <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                      na.strings = c("#DIV/0!", "NA", ""))
testData <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                     na.strings = c("#DIV/0!", "NA", ""))
```
We also load the packages that will be needed for the analysis:
```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```
In looking at the columns of both the training and the test datasets, it appears that they contain the same variables except the test data set does not contain the classe variable (the variable we will be predicting) and contains a problem_id variable instead.

### Data Citation
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz3zvU7UUeS

##Features
After reading in the data, we need to do some preprocessing and data cleanup, ensuring we make the same changes to both the training and test datasets.

While evaluating missing data, I found that either the columns did not have any NAs present or they had more than 97% NA.  I dropped these columns that contained more than 97% missing data.  These variables are not valuable in making the predictions.  

```{r, echo=FALSE}
#Find number of NAs for each column in the dataset - data exploration
na_count <- sapply(trainData, function(y) sum(length(which(is.na(y)))))
notna_count <- sapply(trainData, function(y) sum(length(which(!is.na(y)))))
percentNA <- na_count/(na_count+notna_count) * 100
count <- data.frame(na_count, notna_count, percentNA)
```

```{r}
#only include columns from datasets where columns do not contain NAs
trainData <- trainData[, colSums(is.na(trainData)) == 0]
testData <- testData[, colSums(is.na(testData)) == 0]
```
We also take a look to see if any of the columns are of near zero variance.  These won't have a significant impact on the prediction model.

We see that the 6th column, new_window, has near zero variance and remove that from the dataset.
```{r}
nzv <- nearZeroVar(trainData)
trainData <- trainData[, -nzv]
testData <- testData[, -nzv]
```
We also remove some of the descriptive columns that don't add to the analysis (such as raw timestamps and user names).
```{r}
trainData <- trainData[, -c(1:5)]
testData <- testData[, -c(1:5)]
```
##Algorithm
To prevent overfitting, we split the training dataset into a training and validation test set, using 60% for training and 40% for cross validation.

```{r}
set.seed(1234) 
inTrain <- createDataPartition(trainData$classe, p = 0.6, list = FALSE)
train <- trainData[inTrain, ]
validate <- trainData[-inTrain, ]
```
##Develop Prediction Models

###Decision Tree Model
The first model I look at is a decision tree.

```{r}
##develop model
modelFit1 <- train(classe ~ ., data=train, method="rpart", trControl=trainControl(method = "cv"))
##show tree
fancyRpartPlot(modelFit1$finalModel)
##fit model to the validation dataset
predictionsModFit1 <- predict(modelFit1, validate)
##look at confusion matrix to look at accuracy
confusionMatrix(predictionsModFit1, validate$classe)
```
We can see from the confusion matrix that our accuracy is not very good with accuracy value being around 49%.  

##Random Forest
Next, I will try a random forest to see if that improves accuracy.

```{r}
##develop model
modelFit2 <- train(classe ~ ., data=train, method="rf", trControl=trainControl(method = "cv"))
##fit model to the validation dataset
predictionsModFit2 <- predict(modelFit2, validate)
##look at confusion matrix to look at accuracy
confusionMatrix(predictionsModFit2, validate$classe)
```

Looking at the confusion matrix for the random forest model, we see that accuracy has increased to .9971.  The out of sample error is 1-accuracy, which is .0029 meaning out of sample error is less than .3%.

##Final Prediction
I apply the random forest prediction model to the test data set.
```{r}
predictTest <- predict(modelFit2, testData[, -54])
predictTest
```
##Conclusion
The random forest model provided the best prediction with highest accuracy.  Some tweaks should be made since it does take quite some time to train this model.
